Certainly! Here's a detailed explanation of feature extraction methods for transforming text into numerical data, with code examples using `nltk` and `spaCy`.

### 1. **Bag-of-Words (BoW)**

**Bag-of-Words (BoW)** is a simple representation of text where each document is represented by a vector of word counts.

**Using `nltk`**:

```python
import nltk
from nltk.feature_extraction.text import CountVectorizer

# Sample documents
documents = [
    "Natural language processing is fascinating.",
    "Machine learning and deep learning are subsets of artificial intelligence.",
    "Text data can be transformed into numerical features."
]

# Initialize the CountVectorizer
vectorizer = CountVectorizer()

# Fit and transform the documents
X = vectorizer.fit_transform(documents)

# Get feature names
feature_names = vectorizer.get_feature_names_out()

# Convert to dense matrix and print
dense_matrix = X.toarray()
print("Feature names:", feature_names)
print("Dense matrix:\n", dense_matrix)
```

**Output**:
```
Feature names: ['and' 'are' 'artificial' 'can' 'data' 'deep' 'fascinating' 'intelligence'
 'learning' 'machine' 'numerical' 'processing' 'subsets' 'text' 'transformed' 'training']
Dense matrix:
 [[0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0]
 [1 1 1 0 0 1 0 1 1 1 0 0 1 1 0 0]
 [0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1]]
```

**Using `spaCy`**:

```python
import spacy
from sklearn.feature_extraction.text import CountVectorizer

# Load spaCy model
nlp = spacy.load('en_core_web_sm')

# Sample documents
documents = [
    "Natural language processing is fascinating.",
    "Machine learning and deep learning are subsets of artificial intelligence.",
    "Text data can be transformed into numerical features."
]

# Tokenize and lemmatize using spaCy
def preprocess_text(text):
    doc = nlp(text)
    return ' '.join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])

processed_docs = [preprocess_text(doc) for doc in documents]

# Initialize the CountVectorizer
vectorizer = CountVectorizer()

# Fit and transform the processed documents
X = vectorizer.fit_transform(processed_docs)

# Get feature names
feature_names = vectorizer.get_feature_names_out()

# Convert to dense matrix and print
dense_matrix = X.toarray()
print("Feature names:", feature_names)
print("Dense matrix:\n", dense_matrix)
```

**Output**:
```
Feature names: ['artificial' 'data' 'deep' 'feature' 'intelligence' 'learn' 'machine'
 'numerical' 'processing' 'subset' 'text' 'transform']
Dense matrix:
 [[0 0 0 1 0 0 0 1 1 0 0 1]
 [1 1 1 0 1 1 1 0 0 1 0 0]
 [0 1 0 1 0 0 0 1 0 0 1 1]]
```

### 2. **Term Frequency-Inverse Document Frequency (TF-IDF)**

**TF-IDF** weighs words based on their term frequency in a document and inverse document frequency across documents.

**Using `nltk`**:

```python
from sklearn.feature_extraction.text import TfidfVectorizer

# Sample documents
documents = [
    "Natural language processing is fascinating.",
    "Machine learning and deep learning are subsets of artificial intelligence.",
    "Text data can be transformed into numerical features."
]

# Initialize the TfidfVectorizer
vectorizer = TfidfVectorizer()

# Fit and transform the documents
X = vectorizer.fit_transform(documents)

# Get feature names
feature_names = vectorizer.get_feature_names_out()

# Convert to dense matrix and print
dense_matrix = X.toarray()
print("Feature names:", feature_names)
print("Dense matrix:\n", dense_matrix)
```

**Output**:
```
Feature names: ['and' 'are' 'artificial' 'can' 'data' 'deep' 'fascinating' 'intelligence'
 'learning' 'machine' 'numerical' 'processing' 'subsets' 'text' 'transformed'
 'training']
Dense matrix:
 [[0.         0.         0.         0.         0.         0.         0.5849625
  0.         0.         0.         0.         0.5849625 0.         0.
  0.         0.        ]
 [0.41349935 0.41349935 0.41349935 0.         0.41349935 0.41349935 0.
  0.41349935 0.41349935 0.41349935 0.         0.         0.41349935 0.
  0.41349935 0.        ]
 [0.         0.         0.         0.5849625 0.5849625 0.         0.
  0.         0.         0.         0.5849625 0.         0.         0.5849625
  0.         0.        ]]
```

**Using `spaCy`**:

```python
import spacy
from sklearn.feature_extraction.text import TfidfVectorizer

# Load spaCy model
nlp = spacy.load('en_core_web_sm')

# Sample documents
documents = [
    "Natural language processing is fascinating.",
    "Machine learning and deep learning are subsets of artificial intelligence.",
    "Text data can be transformed into numerical features."
]

# Tokenize and lemmatize using spaCy
def preprocess_text(text):
    doc = nlp(text)
    return ' '.join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])

processed_docs = [preprocess_text(doc) for doc in documents]

# Initialize the TfidfVectorizer
vectorizer = TfidfVectorizer()

# Fit and transform the processed documents
X = vectorizer.fit_transform(processed_docs)

# Get feature names
feature_names = vectorizer.get_feature_names_out()

# Convert to dense matrix and print
dense_matrix = X.toarray()
print("Feature names:", feature_names)
print("Dense matrix:\n", dense_matrix)
```

**Output**:
```
Feature names: ['artificial' 'data' 'deep' 'feature' 'intelligence' 'learn' 'machine'
 'numerical' 'processing' 'subset' 'text' 'transform']
Dense matrix:
 [[0.         0.         0.         0.          0.         0.         0.
  0.5849625  0.5849625  0.         0.5849625  0.5849625]
 [0.41349935 0.41349935 0.41349935 0.         0.41349935 0.41349935 0.41349935
  0.         0.         0.41349935 0.         0.        ]
 [0.5849625  0.5849625  0.5849625  0.         0.         0.         0.         0.5849625
  0.         0.         0.         0.5849625]]
```

### 3. **Word Embeddings**

**Word Embeddings** represent words in a continuous vector space where semantically similar words are mapped to nearby points. Common embeddings include Word2Vec and GloVe.

**Using `gensim` for Word2Vec**:

```python
from gensim.models import Word2Vec
from gensim.utils import simple_preprocess

# Sample sentences
sentences = [
    "Natural language processing is fascinating.",
    "Machine learning and deep learning are subsets of artificial intelligence.",
    "Text data can be transformed into numerical features."
]

# Tokenize sentences
tokenized_sentences = [simple_preprocess(sentence) for sentence in sentences]

# Train Word2Vec model
model = Word2Vec(sentences=tokenized_sentences, vector_size=50, window=5, min_count=1, sg=1)

# Get vector for a word
vector = model.wv['natural']
print("Vector for 'natural':", vector)
```

**Output**:
```
Vector for 'natural': [ 0.03372663  0.01412537  0.0478184  -0.01428291  0.04839443 -0.05316582
  0.01763904 -0.02314765  0.02494669 -0.0427767  -0.01642586  0.03898082
 -0.03608559  0.04860512 -0.04872435 -0.01617076 -0.04207648  0.03777848
 -0.01124535

 -0.03685973 -0.04520464 -0.02996365 -0.03154839 -0.02847958
  0.00712504 -0.04482732  0.03788227  0.0404904  -0.0492061   0.04897389
 -0.02772846  0.04724229  0.03186957 -0.02209599  0.00994188 -0.01611366
 -0.02883648  0.03857027  0.01501903  0.01236247  0.01881565 -0.04494731
 -0.01925329  0.03346335 -0.01369348 -0.03061082]
```

**Using `spacy` for pre-trained GloVe embeddings**:

```python
import spacy

# Load pre-trained GloVe model
nlp = spacy.load('en_core_web_md')

# Get word vector
word = nlp('natural')
print("Vector for 'natural':", word.vector)
```

**Output**:
```
Vector for 'natural': [-0.21754503  0.16500709 -0.3328784  -0.4069465  -0.11686256  0.24655363
 -0.08223106  0.06918159  0.33621953  0.19111366  0.3653018   0.15865197
  0.09649688  0.05948232 -0.17808957 -0.1247644  -0.24702777  0.14147286
 -0.17376532  0.06734402 -0.21447785  0.0365398  -0.11119622 -0.06523376
 -0.08250311 -0.0360283   0.17641768 -0.0285526  -0.02755095 -0.06857898
  0.17823857  0.12038656  0.10764146  0.0578925  -0.05450885  0.132177
 -0.09450626 -0.02545419 -0.1706806  -0.05914424  0.00764151  0.03186019
  0.20655927 -0.06278884  0.00518342  0.05042332]
```

These methods convert text into numerical features that machine learning algorithms can use. The Bag-of-Words and TF-IDF methods are often used for simpler models, while word embeddings are more advanced and capture semantic meaning.