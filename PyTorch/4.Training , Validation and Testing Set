When building and evaluating machine learning models, data is typically split into three distinct sets: training, validation, and testing. Each set serves a different purpose in the model development process.

### Training Set

**Purpose**: Used to train the model.

- **Function**: The model learns from this dataset by adjusting its weights based on the input data and the corresponding target values (labels).
- **Size**: Typically the largest portion of the data, often around 60-80% of the total dataset.
- **Use**: The model is directly exposed to this data during the training process, and the training loss is computed on this set.

### Validation Set

**Purpose**: Used to tune hyperparameters and validate the model during training.

- **Function**: Helps monitor the model's performance on unseen data during training to prevent overfitting. It is used to validate the model and adjust hyperparameters like learning rate, number of layers, etc.
- **Size**: Usually about 10-20% of the total dataset.
- **Use**: The model is not trained on this set, but it is used to evaluate the model's performance periodically during training. Metrics like validation loss and accuracy are monitored to decide when to stop training (early stopping) and to select the best model parameters.

### Testing Set

**Purpose**: Used to evaluate the final model's performance.

- **Function**: Provides an unbiased evaluation of the model after training. It is used to estimate how well the model will perform on real-world data.
- **Size**: Typically around 10-20% of the total dataset.
- **Use**: The model is evaluated on this set only once, after training is complete, to report the final performance metrics like accuracy, precision, recall, F1 score, etc.

### Example Split

For a dataset with 10,000 samples:
- **Training Set**: 7,000 samples (70%)
- **Validation Set**: 1,500 samples (15%)
- **Testing Set**: 1,500 samples (15%)

### Practical Example in PyTorch

#### Step 1: Load and Split the Dataset

```python
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, Subset

# Assuming 'dataset' is a PyTorch Dataset object
train_idx, test_idx = train_test_split(list(range(len(dataset))), test_size=0.2, random_state=42)
train_idx, val_idx = train_test_split(train_idx, test_size=0.2, random_state=42)

train_dataset = Subset(dataset, train_idx)
val_dataset = Subset(dataset, val_idx)
test_dataset = Subset(dataset, test_idx)
```

#### Step 2: Create DataLoaders

```python
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
```

#### Step 3: Training Loop with Validation

```python
for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        train_loss += loss.item() * inputs.size(0)
    
    train_loss /= len(train_loader.dataset)
    
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for inputs, labels in val_loader:
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            val_loss += loss.item() * inputs.size(0)
    
    val_loss /= len(val_loader.dataset)
    
    print(f'Epoch {epoch+1}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')
```

#### Step 4: Testing the Model

```python
model.eval()
test_loss = 0.0
with torch.no_grad():
    for inputs, labels in test_loader:
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        test_loss += loss.item() * inputs.size(0)
    
test_loss /= len(test_loader.dataset)
print(f'Test Loss: {test_loss:.4f}')
```

### Summary

- **Training Set**: Used for model training.
- **Validation Set**: Used for hyperparameter tuning and to monitor the model during training.
- **Testing Set**: Used for final evaluation of the model.

Splitting your data into these three sets ensures that you can train a robust model and evaluate its performance in a way that generalizes well to new, unseen data.