In PyTorch, optimizers are used to update the model parameters based on the computed gradients during training.
They play a crucial role in the optimization process by adjusting the learning rate and other hyperparameters 
to improve the model's performance. Here's a detailed overview of PyTorch optimizers and how to use them:

### Common Optimizers in PyTorch

1. **Stochastic Gradient Descent (SGD)**
2. **Adam**
3. **RMSprop**
4. **Adagrad**
5. **Adadelta**

### Usage of Optimizers

#### Step-by-Step Example

1. **Define the Model**:
   - Create a neural network model.

```python
import torch
import torch.nn as nn

class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc = nn.Linear(10, 2)  # Example model with one fully connected layer

    def forward(self, x):
        return self.fc(x)

model = SimpleModel()
```

2. **Choose an Optimizer**:
   - Select an optimizer and initialize it with model parameters and hyperparameters.

```python
import torch.optim as optim

optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
# Other optimizers can be used similarly:
# optimizer = optim.Adam(model.parameters(), lr=0.001)
# optimizer = optim.RMSprop(model.parameters(), lr=0.01)
```

3. **Define a Loss Function**:
   - Choose an appropriate loss function for your task.

```python
criterion = nn.CrossEntropyLoss()
```

4. **Training Loop**:
   - Implement the training loop where you perform forward and backward passes, and update the model parameters using the optimizer.

```python
# Sample data
inputs = torch.randn(5, 10)  # Batch of 5 samples, each with 10 features
labels = torch.randint(0, 2, (5,))  # Batch of 5 labels for binary classification

# Training loop
for epoch in range(100):  # Number of epochs
    optimizer.zero_grad()  # Zero the gradients before the backward pass
    
    outputs = model(inputs)  # Forward pass
    loss = criterion(outputs, labels)  # Compute loss
    
    loss.backward()  # Backward pass to compute gradients
    optimizer.step()  # Update model parameters
    
    print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')
```

### Explanation

- **optimizer.zero_grad()**: Clears old gradients from the last step (otherwise they would accumulate).
- **outputs = model(inputs)**: Computes the model's predictions.
- **loss = criterion(outputs, labels)**: Computes the loss between the predictions and true labels.
- **loss.backward()**: Computes the gradient of the loss w.r.t. the model parameters.
- **optimizer.step()**: Updates the model parameters based on the gradients.

### Common Optimizers in Detail

1. **SGD (Stochastic Gradient Descent)**:
   - Basic form of gradient descent.
   - `optim.SGD(parameters, lr=learning_rate, momentum=momentum_value)`

2. **Adam (Adaptive Moment Estimation)**:
   - Combines the advantages of two other extensions of stochastic gradient descent.
   - `optim.Adam(parameters, lr=learning_rate, betas=(beta1, beta2), eps=epsilon)`

3. **RMSprop (Root Mean Square Propagation)**:
   - Addresses the issue of rapidly decreasing learning rates.
   - `optim.RMSprop(parameters, lr=learning_rate, alpha=decay_rate, eps=epsilon)`

4. **Adagrad (Adaptive Gradient Algorithm)**:
   - Adaptively changes the learning rate.
   - `optim.Adagrad(parameters, lr=learning_rate, lr_decay=learning_rate_decay)`

5. **Adadelta**:
   - Extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate.
   - `optim.Adadelta(parameters, lr=learning_rate, rho=decay_rate, eps=epsilon)`

### Choosing an Optimizer
- **SGD**: Good for large-scale and sparse data.
- **Adam**: Often works well for a wide variety of models and problems.
- **RMSprop**: Useful for recurrent neural networks.
- **Adagrad**: Suitable for sparse data.
- **Adadelta**: Useful when adapting learning rates in a more robust way than Adagrad.

Selecting the right optimizer and its parameters is crucial and often involves experimentation and hyperparameter tuning.