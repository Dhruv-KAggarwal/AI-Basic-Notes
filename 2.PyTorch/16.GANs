Generative Adversarial Networks (GANs) are a class of neural networks used for generative tasks. They consist of two main components: a **Generator** and a **Discriminator**. Hereâ€™s a step-by-step guide to implementing a simple GAN in PyTorch:

### 1. **Define the Generator and Discriminator Models**

**Generator**: Takes a random noise vector as input and generates data (e.g., images).

**Discriminator**: Takes data as input and outputs a probability indicating whether the data is real or generated.

#### **Generator**

```python
import torch
import torch.nn as nn

class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(100, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, 1024),
            nn.ReLU(),
            nn.Linear(1024, 28*28),
            nn.Tanh()  # Output layer with Tanh for image pixel values in range [-1, 1]
        )

    def forward(self, x):
        return self.fc(x).view(-1, 1, 28, 28)  # Reshape to image dimensions
```

#### **Discriminator**

```python
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(28*28, 1024),
            nn.LeakyReLU(0.2),
            nn.Linear(1024, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 1),
            nn.Sigmoid()  # Output layer with Sigmoid for probability
        )

    def forward(self, x):
        x = x.view(-1, 28*28)  # Flatten the image
        return self.fc(x)
```

### 2. **Prepare the Data**

Load and preprocess the data (e.g., MNIST dataset):

```python
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# Define transformations
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

# Load dataset
dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)
data_loader = DataLoader(dataset, batch_size=64, shuffle=True)
```

### 3. **Define the Training Loop**

Set up the loss functions and optimizers:

```python
import torch.optim as optim

# Initialize models
generator = Generator()
discriminator = Discriminator()

# Loss function and optimizers
criterion = nn.BCELoss()  # Binary Cross-Entropy Loss
optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))
```

#### **Training Loop**

```python
import numpy as np

def train_gan(generator, discriminator, data_loader, num_epochs=5):
    for epoch in range(num_epochs):
        for real_images, _ in data_loader:
            batch_size = real_images.size(0)
            real_labels = torch.ones(batch_size, 1)  # Labels for real images
            fake_labels = torch.zeros(batch_size, 1)  # Labels for fake images

            # Train Discriminator
            optimizer_d.zero_grad()
            
            # Real images
            outputs = discriminator(real_images)
            d_loss_real = criterion(outputs, real_labels)
            d_loss_real.backward()
            
            # Fake images
            noise = torch.randn(batch_size, 100)
            fake_images = generator(noise)
            outputs = discriminator(fake_images.detach())
            d_loss_fake = criterion(outputs, fake_labels)
            d_loss_fake.backward()
            optimizer_d.step()
            
            # Train Generator
            optimizer_g.zero_grad()
            
            outputs = discriminator(fake_images)
            g_loss = criterion(outputs, real_labels)  # Want the generator to fool the discriminator
            g_loss.backward()
            optimizer_g.step()
            
        print(f'Epoch [{epoch+1}/{num_epochs}], d_loss: {d_loss_real.item() + d_loss_fake.item()}, g_loss: {g_loss.item()}')

# Train the GAN
train_gan(generator, discriminator, data_loader)
```

### 4. **Generate and Save Images**

After training, you can use the generator to create new images.

```python
import matplotlib.pyplot as plt

def generate_images(generator, num_images=16):
    noise = torch.randn(num_images, 100)
    with torch.no_grad():
        generated_images = generator(noise)
    
    # Convert images to numpy and display
    generated_images = generated_images.numpy()
    generated_images = np.transpose(generated_images, (0, 2, 3, 1))  # Change dimensions to (N, H, W, C)
    
    fig, axs = plt.subplots(4, 4, figsize=(8, 8))
    idx = 0
    for i in range(4):
        for j in range(4):
            axs[i, j].imshow(generated_images[idx, 0], cmap='gray')
            axs[i, j].axis('off')
            idx += 1
    plt.show()

# Generate and display images
generate_images(generator)
```

### Summary

1. **Define Models**: Create the Generator and Discriminator networks.
2. **Prepare Data**: Load and preprocess the dataset.
3. **Training Loop**: Implement the training procedure for both Generator and Discriminator.
4. **Generate Images**: Use the trained Generator to create new images.

This example demonstrates a simple GAN using fully connected layers. 
For more complex tasks, like generating high-resolution images, you might use convolutional layers or more advanced architectures like DCGAN or WGAN. Let me know if you need further details or have specific questions!